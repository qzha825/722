{"cells":[{"cell_type":"markdown","metadata":{"id":"Z4ZbRwm_NbFc"},"source":["# Spark DataFrame - Basics\n","\n","Let's start off with the fundamentals of Spark DataFrame.\n","\n","Objective: In this exercise, you'll find out how to start a spark session, read in data, explore the data and manipuluate the data (using DataFrame syntax as well as SQL syntax). Let's get started!"]},{"cell_type":"code","source":["!apt-get install openjdk-11-jdk-headless -qq\n","!pip install -q pyspark findspark"],"metadata":{"id":"s3lYusO-OWB7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n","# No need to download Sparkâ€”pip installation includes Spark JARs\n","\n","import findspark\n","findspark.init()\n","\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName(\"ColabSpark\").getOrCreate()\n","spark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219},"id":"57Xb5juLOZIf","executionInfo":{"status":"ok","timestamp":1754347156276,"user_tz":-720,"elapsed":48,"user":{"displayName":"Ruhi Bajaj","userId":"03141995459803611728"}},"outputId":"b5b5230b-ee93-49d3-b0a8-3bf2f88b5557"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pyspark.sql.session.SparkSession at 0x7d3c0c9c8a10>"],"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://18d0a8fd8c98:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.5.1</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>ColabSpark</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5WrG2dT2O1Z7","executionInfo":{"status":"ok","timestamp":1754347141006,"user_tz":-720,"elapsed":3655,"user":{"displayName":"Ruhi Bajaj","userId":"03141995459803611728"}},"outputId":"ba785222-d9a5-4ca2-afe7-ed82eb0ab0d0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dXqLhFY_NbFj","outputId":"c73d9305-841e-4fbb-f2a1-a91ae858a0d0"},"outputs":[{"name":"stderr","output_type":"stream","text":["\r","[Stage 0:>                                                          (0 + 1) / 1]\r","\r","                                                                                \r"]}],"source":["# Let's read in the data. Note that it's in the format of JSON.\n","#change the path\n","df = spark.read.json('Datasets/people.json')"]},{"cell_type":"markdown","metadata":{"id":"2M-nBR2INbFj"},"source":["## Data Exploration"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Z0GYO7nNbFj","executionInfo":{"status":"ok","timestamp":1754347196297,"user_tz":-720,"elapsed":1035,"user":{"displayName":"Ruhi Bajaj","userId":"03141995459803611728"}},"outputId":"fcfe7bc3-75b6-4124-f878-feb572f7774c"},"outputs":[{"output_type":"stream","name":"stdout","text":["+----+-------+\n","| age|   name|\n","+----+-------+\n","|NULL|Michael|\n","|  30|   Andy|\n","|  19| Justin|\n","+----+-------+\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["['age', 'name']"]},"metadata":{},"execution_count":10}],"source":["# The show method allows you visualise DataFrames. We can see that there are two columns.\n","df.show()\n","\n","# You could also try this.\n","df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6FGkY3UGNbFk","executionInfo":{"status":"ok","timestamp":1754347200279,"user_tz":-720,"elapsed":2273,"user":{"displayName":"Ruhi Bajaj","userId":"03141995459803611728"}},"outputId":"5f062737-f726-4e1c-c57a-b0d60f0cebac"},"outputs":[{"output_type":"stream","name":"stdout","text":["+-------+------------------+-------+\n","|summary|               age|   name|\n","+-------+------------------+-------+\n","|  count|                 2|      3|\n","|   mean|              24.5|   NULL|\n","| stddev|7.7781745930520225|   NULL|\n","|    min|                19|   Andy|\n","|    max|                30|Michael|\n","+-------+------------------+-------+\n","\n"]}],"source":["# We can use the describe method get some general statistics on our data too. Remember to show the DataFrame!\n","# But what about data type?\n","df.describe().show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pm3GsbefNbFk","executionInfo":{"status":"ok","timestamp":1754347201495,"user_tz":-720,"elapsed":38,"user":{"displayName":"Ruhi Bajaj","userId":"03141995459803611728"}},"outputId":"f99e2c41-2973-4184-a1eb-613a254f7c18"},"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- age: long (nullable = true)\n"," |-- name: string (nullable = true)\n","\n"]}],"source":["# For type, we can use print schema.\n","# But wait! What if you want to change the format of the data? Maybe change age to an integer instead of long?\n","df.printSchema()"]},{"cell_type":"markdown","metadata":{"id":"eIT2_IceNbFk"},"source":["## Data Manipulation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VgVQTDNjNbFk"},"outputs":[],"source":["# Let's import in the relevant types.\n","from pyspark.sql.types import (StructField,StringType,IntegerType,StructType)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OLtEumX0NbFl"},"outputs":[],"source":["# Then create a variable with the correct structure.\n","data_schema = [StructField('age',IntegerType(),True),\n","              StructField('name',StringType(),True)]\n","\n","final_struct = StructType(fields=data_schema)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qI-OlnWNNbFl","executionInfo":{"status":"ok","timestamp":1754347219512,"user_tz":-720,"elapsed":55,"user":{"displayName":"Ruhi Bajaj","userId":"03141995459803611728"}},"outputId":"65c342bb-18b0-4970-8fff-f280605c8cc8"},"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- age: integer (nullable = true)\n"," |-- name: string (nullable = true)\n","\n"]}],"source":["# And now we can read in the data using that schema. If we print the schema, we can see that age is now an integer.\n","df = spark.read.json('/content/drive/MyDrive/Infosys722/Datasets/people.json', schema=final_struct)\n","\n","df.printSchema()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZmG6u0LENbFl","executionInfo":{"status":"ok","timestamp":1754347223766,"user_tz":-720,"elapsed":1015,"user":{"displayName":"Ruhi Bajaj","userId":"03141995459803611728"}},"outputId":"332638df-9176-48a5-aa78-92d16ce4f6be"},"outputs":[{"output_type":"stream","name":"stdout","text":["+----+\n","| age|\n","+----+\n","|NULL|\n","|  30|\n","|  19|\n","+----+\n","\n","+----+\n","| age|\n","+----+\n","|NULL|\n","|  30|\n","|  19|\n","+----+\n","\n"]}],"source":["# We can also select various columns from a DataFrame.\n","df.select('age').show()\n","\n","# We could split up these steps, first assigning the output to a variable, then showing that variable. As you see, the output is the same.\n","ageColumn = df.select('age')\n","\n","ageColumn.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5TuCL6dXNbFl","executionInfo":{"status":"ok","timestamp":1754347227510,"user_tz":-720,"elapsed":585,"user":{"displayName":"Ruhi Bajaj","userId":"03141995459803611728"}},"outputId":"8d187871-64e2-4c43-be86-d5db864e65e2"},"outputs":[{"output_type":"stream","name":"stdout","text":["+----+-------+----------+\n","| age|   name|double_age|\n","+----+-------+----------+\n","|NULL|Michael|      NULL|\n","|  30|   Andy|        60|\n","|  19| Justin|        38|\n","+----+-------+----------+\n","\n","+----+-------+\n","| age|   name|\n","+----+-------+\n","|NULL|Michael|\n","|  30|   Andy|\n","|  19| Justin|\n","+----+-------+\n","\n"]}],"source":["# We can also add columns, manipulating the DataFrame.\n","\n","df.withColumn('double_age',df['age']*2).show()\n","\n","# But note that this doesn't alter the original DataFrame. You need to assign the output to a new variable in order to do so.\n","df.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Itb8rxUaNbFl","executionInfo":{"status":"ok","timestamp":1754347230259,"user_tz":-720,"elapsed":432,"user":{"displayName":"Ruhi Bajaj","userId":"03141995459803611728"}},"outputId":"3337978b-5966-44f1-d2d6-4ef91ceaa3d4"},"outputs":[{"output_type":"stream","name":"stdout","text":["+----------+-------+\n","|my_new_age|   name|\n","+----------+-------+\n","|      NULL|Michael|\n","|        30|   Andy|\n","|        19| Justin|\n","+----------+-------+\n","\n"]}],"source":["# We can rename columns too!\n","df.withColumnRenamed('age', 'my_new_age').show()"]},{"cell_type":"markdown","metadata":{"id":"f6zZmDsUNbFm"},"source":["## Introducing SQL\n","We can query a DataFrame as if it were a table! Let's see a few examples of that below:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NtkNsXvuNbFm"},"outputs":[],"source":["# First, we have to register the DataFrame as a SQL temporary view.\n","df.createOrReplaceTempView('people')\n","\n","# After that, we can use the SQL programming language for queries.\n","results = spark.sql(\"SELECT * FROM people\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kyWtmM3eNbFm","executionInfo":{"status":"ok","timestamp":1754347249965,"user_tz":-720,"elapsed":435,"user":{"displayName":"Ruhi Bajaj","userId":"03141995459803611728"}},"outputId":"acb57c5e-dcdc-45a6-a116-1c954a101faa"},"outputs":[{"output_type":"stream","name":"stdout","text":["+---+\n","|age|\n","+---+\n","| 30|\n","| 19|\n","+---+\n","\n"]}],"source":["# Here's another example:\n","results = spark.sql(\"SELECT age FROM people WHERE age >= 19\")\n","results.show()"]},{"cell_type":"markdown","metadata":{"id":"90rn9IzpNbFm"},"source":["Now that we're done with this tutorial, let's move on to Spark DataFrame Operations!"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}